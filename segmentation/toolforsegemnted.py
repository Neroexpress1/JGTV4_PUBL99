# -*- coding: utf-8 -*-
"""toolforsegemnted.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TJB_wFKAd8bFins0myczGYoHZZMfDk98
"""

!pip install pythainlp
!pip install pandas

import pandas as pd
import io
from google.colab import files
from pythainlp.tokenize import word_tokenize
from pythainlp.util import Trie #for custom dictionaries to better segment the text

def upload_files():
    uploaded = files.upload()
    return uploaded

# Upload and read the dataset
print("Upload your CSV data file:")
uploaded_data = upload_files()
df = pd.read_csv(io.BytesIO(uploaded_data[next(iter(uploaded_data))]))

# Upload and create a Trie from the custom dictionary
print("Upload your custom dictionary file:")
uploaded_custom_dict = upload_files()
uploaded_custom_dict_path = next(iter(uploaded_custom_dict))

with open(uploaded_custom_dict_path, 'r', encoding='utf-8') as f:
    custom_words = f.read().splitlines()

custom_trie = Trie(custom_words)  # Initialize Trie with custom words

# Define the custom tokenization function
def custom_word_tokenize(text, custom_dict):
    return word_tokenize(text, custom_dict=custom_dict, engine="mm", keep_whitespace=False, join_broken_num=True)

# Apply custom tokenization to the 'Speech' column
df['segmented_speech'] = df['Speech'].apply(lambda x: custom_word_tokenize(x, custom_trie))

# verify the changes
print(df.head())

# Save the DataFrame to a new CSV file and prompt download
output_filename = 'segmented_data_16_year2555.csv'
df.to_csv(output_filename, index=False)
files.download(output_filename)